1st program : handtrack_min.py

2nd program : mainproj.py

Hand tracking is a computer vision technique used to detect and track human hands in images or video streams. The MediaPipe library provides an efficient and pre-trained hand-tracking solution, which identifies key points (landmarks) on the hand. These landmarks allow for gesture recognition and hand pose estimation.

##Key Concepts

1. Landmarks
A landmark is a specific point on the hand, such as the tip of a finger, the base of the palm, or a joint.
MediaPipe uses 21 landmarks for each hand. Each landmark has:
ID: A unique identifier for each point (e.g., the tip of the index finger is 8).
x, y, z coordinates:
x and y are normalized coordinates (values between 0 and 1), which are scaled to image width and height to get pixel positions.
z is the depth relative to the hand's center.

2. Hand Model
The landmarks represent a skeletal structure of the hand, including:

Fingers (thumb, index, middle, ring, pinky)
Joints (base, middle, and tip of each finger)
Palm points

3. Hand Tracking
Hand tracking involves:

Detection:

Identifying where the hand is in the image or video frame.
Using a bounding box or key landmarks to locate the hand.
Tracking:
Following the hand across consecutive frames for smooth real-time interaction.
Landmark Estimation:
Predicting and marking the 21 landmarks on the detected hand.

4. Drawing Landmarks and Connections
MediaPipe provides utility functions to draw:

Landmarks:
Small circles or points that represent each detected key location.

Connections:
Lines connecting related landmarks, forming the skeletal structure of the hand.
Example: A line between the base and tip of the index finger.
How MediaPipe Hand Tracking Works

Image Preprocessing:
Converts the input image from BGR (used by OpenCV) to RGB (used by MediaPipe).

Model Inference:
Uses a neural network model to detect hands and predict their landmarks.

Landmark Normalization:
Converts landmark coordinates into normalized values relative to the image dimensions.

Post-Processing:
Maps the normalized values back to pixel positions for drawing and interaction.

Drawing Landmarks and Connections

Drawing Landmarks:

for id, lm in enumerate(myHand.landmark):
    h, w, c = img.shape
    cx, cy = int(lm.x * w), int(lm.y * h)  # Convert normalized values to pixels
    cv2.circle(img, (cx, cy), 10, (255, 0, 255), cv2.FILLED)
cv2.circle: Draws a circle at each landmark position on the image.
Parameters:
(cx, cy): Pixel coordinates of the landmark.
10: Circle radius.
(255, 0, 255): Color (purple in BGR format).

Drawing Connections:

mpDraw.draw_landmarks(img, handLms, mpHands.HAND_CONNECTIONS)
HAND_CONNECTIONS: A predefined set of pairs of landmarks that should be connected (e.g., from the base to the tip of a finger).
Uses the MediaPipe utility function to handle connections automatically.

Example: Understanding the Finger Skeleton
Each finger has specific landmark IDs:

Thumb: 1, 2, 3, 4 (base to tip)
Index: 5, 6, 7, 8
Middle: 9, 10, 11, 12
Ring: 13, 14, 15, 16
Pinky: 17, 18, 19, 20
The connections form a tree-like structure:

Palm landmarks are connected to the base of each finger.
Finger joints are connected from base to tip.

##1st prog explanation:
The code in `handtrack_min.py` is a custom Python module that uses the **MediaPipe** library for hand tracking and provides an easy-to-use class, `handDetector`, to detect hands, draw hand landmarks, and return the positions of landmarks in an image.

---

### **What the Code Does**
The code performs three main functions:
1. **Initialize MediaPipe Hand Tracking**:
   - Sets up MediaPipe’s hand-tracking solution with customizable parameters like detection confidence and tracking confidence.
   
2. **Detect Hands in Images**:
   - Detects hands in an input image and optionally draws the detected hand landmarks on the image.

3. **Return Positions of Hand Landmarks**:
   - Returns a list of positions (x, y coordinates) of landmarks for the detected hands.

---

### **Detailed Explanation**

#### 1. **Imports and Setup**
```python
import cv2
import mediapipe as mp
import os
os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'
```
- **`cv2` (OpenCV)**: Handles image processing and video capture.
- **`mediapipe`**: Provides a pre-trained hand-tracking model for detecting and analyzing hand gestures.
- **Environment Variable (`os.environ`)**: Ensures compatibility with some systems when using `mediapipe`.

---

#### 2. **Class Initialization**
```python
class handDetector():
    def __init__(self, mode=False, maxHands=2, detectionCon=0.5, trackCon=0.5):
        self.mode = mode
        self.maxHands = maxHands
        self.detectionCon = detectionCon
        self.trackCon = trackCon
```
- **`mode`**:
  - If `True`, runs the detector in static image mode (good for analyzing individual images).
  - If `False`, processes a video feed dynamically (for real-time hand tracking).
- **`maxHands`**: The maximum number of hands to detect (default is 2).
- **`detectionCon`**: The confidence threshold for detecting hands.
- **`trackCon`**: The confidence threshold for tracking landmarks in detected hands.

---

#### 3. **MediaPipe Hands Initialization**
```python
self.mpHands = mp.solutions.hands
self.hands = self.mpHands.Hands(
    static_image_mode=self.mode,
    max_num_hands=self.maxHands,
    min_detection_confidence=self.detectionCon,
    min_tracking_confidence=self.trackCon
)
self.mpDraw = mp.solutions.drawing_utils
```
- Initializes MediaPipe's **Hands module**.
- **`mp.solutions.hands.Hands`**: Handles the core hand-tracking process.
- **`mp.solutions.drawing_utils`**: A utility to draw hand landmarks and connections on images.

---

#### 4. **Finding Hands**
```python
def findHands(self, img, draw=True):
    imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    self.results = self.hands.process(imgRGB)
```
- Converts the input image from BGR to RGB (MediaPipe works with RGB images).
- Processes the image to detect hands and landmarks.
  
```python
if self.results.multi_hand_landmarks:
    for handLms in self.results.multi_hand_landmarks:
        if draw:
            self.mpDraw.draw_landmarks(img, handLms, self.mpHands.HAND_CONNECTIONS)
```
- **`multi_hand_landmarks`**: Contains the landmarks (points) for each detected hand.
- Optionally draws landmarks and connections between them on the original image.

---

#### 5. **Finding Landmark Positions**
```python
def findPosition(self, img, handNo=0, draw=True):
    lmList = []
    if self.results.multi_hand_landmarks:
        myHand = self.results.multi_hand_landmarks[handNo]
```
- Retrieves landmarks for a specific hand (`handNo`), defaulting to the first detected hand.

```python
for id, lm in enumerate(myHand.landmark):
    h, w, c = img.shape
    cx, cy = int(lm.x * w), int(lm.y * h)
    lmList.append([id, cx, cy])
```
- Each landmark is normalized (values between 0 and 1). These are scaled to the image dimensions (`h`, `w`) to get pixel coordinates.
- **`lmList`**: A list of landmark positions, where each entry is `[id, x, y]`.
  - **`id`**: Landmark ID (e.g., tip of index finger is ID 8).
  - **`x`, `y`**: Pixel coordinates of the landmark on the image.

```python
if draw:
    cv2.circle(img, (cx, cy), 15, (255, 0, 255), cv2.FILLED)
```
- Optionally draws a circle at each landmark position.

---

### **How This Module is Used**
1. **Initialize the Detector**:
   ```python
   detector = handDetector()
   ```

2. **Capture an Image or Video Frame**:
   ```python
   ret, img = cap.read()  # Capture a frame using OpenCV
   ```

3. **Find Hands in the Frame**:
   ```python
   img = detector.findHands(img)  # Detect hands and draw landmarks
   ```

4. **Get Landmark Positions**:
   ```python
   lmList = detector.findPosition(img)  # Get a list of landmark positions
   ```
   - Use the `lmList` to implement custom logic, like recognizing gestures or controlling devices.



##2nd prog explanation:
Here’s a detailed explanation of the **Gesture Volume Control** program, section by section:

---

### **1. Importing Libraries**
```python
import cv2
import mediapipe as mp
import numpy as np
import pyautogui
import time
from handtrack_min import handDetector
```
- **`cv2` (OpenCV)**: Used for image processing and capturing video from the webcam.
- **`mediapipe`**: Provides tools for detecting and tracking hands.
- **`numpy`**: Used for mathematical operations, like calculating the distance between points.
- **`pyautogui`**: Simulates keyboard shortcuts for controlling the system volume.
- **`handtrack_min`**: A custom module you created for hand detection and position tracking.

---

### **2. Hand Detection Initialization**
```python
detector = handDetector()
```
- Initializes an instance of the `handDetector` class.
- The `handDetector` class (from `handtrack_min.py`) is responsible for detecting hand landmarks and providing their positions.

---

### **3. Camera Setup**
```python
cap = cv2.VideoCapture(0)
```
- Opens the primary webcam (`0` indicates the default camera) for capturing video.

---

### **4. Main Loop**
```python
while True:
    ret, img = cap.read()
    if not ret:
        break
```
- **`cap.read()`**: Captures a frame from the webcam.
- If `ret` is `False`, it indicates an issue with the webcam, and the loop exits.

---

### **5. Hand Detection in Each Frame**
```python
img = detector.findHands(img)
lmList = detector.findPosition(img)
```
- **`detector.findHands(img)`**: Detects hands in the current frame and draws landmarks if any are found.
- **`detector.findPosition(img)`**: Returns a list of landmarks (`lmList`) for the detected hand. Each landmark contains an ID and its x, y coordinates.

---

### **6. Pinch Gesture for Volume Control**
```python
if len(lmList) != 0:
    index_tip = lmList[8]  # Index finger tip (id=8)
    thumb_tip = lmList[4]  # Thumb tip (id=4)

    # Calculate the distance between index and thumb tips
    distance = np.sqrt((index_tip[1] - thumb_tip[1])**2 + (index_tip[2] - thumb_tip[2])**2)
```
- **Landmarks Used**:
  - `lmList[8]`: Coordinates of the index finger tip.
  - `lmList[4]`: Coordinates of the thumb tip.
- **Distance Calculation**:
  - Computes the Euclidean distance between the index and thumb tips. This is used to detect the "pinch" gesture (when distance < 30).

---

### **7. Mapping Y-Position to Volume**
```python
y_position = index_tip[2]
new_volume = np.interp(y_position, [0, 480], [0, 100])
volume = int(new_volume)
```
- **`y_position`**: The y-coordinate of the index finger tip.
- **`np.interp()`**:
  - Maps the `y_position` from the range `[0, 480]` (screen height) to `[0, 100]` (volume percentage).
  - Higher y-values correspond to lower volumes and vice versa.
- The result is converted to an integer for simplicity.

---

### **8. Adjusting System Volume**
```python
pyautogui.hotkey('volumedown') if volume < 50 else pyautogui.hotkey('volumeup')
```
- **Volume Control**:
  - If the mapped volume is less than 50%, it simulates pressing the `volumedown` key.
  - Otherwise, it simulates pressing the `volumeup` key.

---

### **9. Displaying Volume Level**
```python
cv2.putText(img, f'Volume: {volume}%', (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
```
- Draws the current volume level on the screen for real-time feedback.
- **`cv2.putText`**: Displays text on the image at a specified position with styling options.

---

### **10. Display the Webcam Feed**
```python
cv2.imshow("Gesture Volume Control", img)
```
- Opens a window displaying the live webcam feed, with detected hands and the current volume level.

---

### **11. Exit Condition**
```python
if cv2.waitKey(1) & 0xFF == ord('q'):
    break
```
- Checks for the `q` key press to exit the loop and close the program.

---

### **12. Cleanup**
```python
cap.release()
cv2.destroyAllWindows()
```
- Releases the webcam resource.
- Closes all OpenCV windows.

##Instructions to Operate the Gesture Volume Control Program:

set up your environment as said in this video:
https://youtu.be/KJepCMc0WMo?si=PL3kAo_Q3Ll4Q6Fm

Install the required libraries by running in that environment:
pip install mediapipe opencv-python pyautogui numpy

Run the program using:
python mainproj.py

Using the Gesture Control:

Ensure your hand is visible to the webcam.
Perform gestures to control the volume:
Pinch Gesture: Bring your thumb and index finger close together (distance < 30 pixels) to activate the volume control.
Move Up/Down: Move your pinched hand up to increase volume or down to decrease volume. The closer your hand is to the top of the screen, the higher the volume, and vice versa.
The current volume percentage will be displayed on the screen.

Press the q key on your keyboard to close the program and release the webcam.
